**Practical Machine Learning**

**End of Course Project- by Corey May**

```{r, echo=FALSE}
suppressMessages(library(ggplot2) )
suppressMessages(library(caret) )
suppressMessages(library(kernlab) )
suppressMessages(library(gridExtra) )
```


Below are the steps I took to complete the end of course project.  This write-up is split into 3 sections data preparation/exploratory data analysis (EDA), modeling and results.


**Data Preparation and EDA**

The first step of the project was data preparation and EDA.  I began by eliminating variables that had insufficient data, 100 out of 160 variables had missing values (NA’s) for 98% of the 19,622 records.   These variables were removed from the analysis in both the training and test sets (see figure 1). 


```{r}
## Read in and prepare data
pmltrain = read.csv("/Users/cmay/Documents/Training/pml-training.csv", stringsAsFactors=FALSE)
pmltest  = read.csv("/Users/cmay/Documents/Training/pml-testing.csv",  stringsAsFactors=FALSE)
  names(pmltrain)[names(pmltrain)=='X'] <- 'id'
  names(pmltest )[names(pmltest) =='X'] <- 'id'
  table(pmltrain$classe, exclude=FALSE)

## Get the number of NA records in a dataframes of columns
countnas <- function(z) {
  na_count <-sapply(z, function(y) sum(length(which(is.na(y) | y == ""))))
  na_count <- data.frame(na_count)
  na_count$pct <- round(na_count$na_count  / nrow(z),4)
  return(na_count)
}

```


Figure 1- excerpt of variables to be removed
```{r}
## Writeout table for report
head(countnas(pmltrain),20)
```



Next, I explored predictors that were not associated with the outcome variable ‘classe’.  I used a series of plotting techniques, boxplots, density plots, histograms, etc. to visualize the relationships (see figure 2).  I determined that all the variables that began with “gyro” could be removed as most of the values for these predictors were near-zero or zero and would not be good classifiers for the outcome variable.

Finally, I investigated whether principal component analysis (PCA) could be used to reduce the number of predictors and/or identify predictors that were correlated.  The results of this did not appear helpful so I did not use PCA (see Figure 3).


```{r}
## Remove vars with at least one NA, most have 19K NA's
## Set sample seed here.  This will be run 3 times by changing the seed to create different samples
## Could also build a function for all of this work if I want to
  ## 1) seed 41;
  ## 2) seed 92621;
  ## 3) seed 52800
set.seed(41) 
pmltrain.clean <- pmltrain[ , apply(pmltrain, 2, function(x) {!any(is.na(x)) & !any( x == "") } )]
pmltest.clean  <- pmltest[ ,  apply(pmltest, 2, function(x) {!any(is.na(x)) & !any( x == "") } )]

## Remove character, id and timestamp variables from final test set
pmltest.clean  <- pmltest.clean[,-c(1:7)]

## Build cross-validation training and test sets
intrain  = createDataPartition(pmltrain.clean$classe, p=3/4)[[1]]
training = pmltrain.clean[ intrain, ]
testing  = pmltrain.clean[-intrain, ]

## Remove indicator, character and time variables from training data
training <- training[,-c(1:7)]

## Prepare 'clean' testing data set for trained models
testing.clean <- testing[ , apply(testing, 2, function(x) {!any(is.na(x)) & !any( x == "") } )]
testing.clean <- testing.clean[,-c(1:7)]
```

Figure 2- boxplot of variable 'gyros_forearm_x', most values near-zero or zero

```{r, echo=FALSE}
p1 <- qplot(classe, training[,"gyros_forearm_y"], fill=classe, data=training, geom=c("boxplot"), ylab="gyros_forearm_y",
            main=paste0("Boxplot of ", "gyros_forearm_y") )

p2 <- qplot(classe, training[,"gyros_forearm_y"], fill=classe, data=training, geom=c("boxplot", "jitter"), ylab="gyros_forearm_y",
            main=paste0("Boxplot of ", "gyros_forearm_y" ) )

grid.arrange(p1,p2,ncol=2)  
```

Figure 3- principal component plot

```{r, echo=FALSE}
typeColor <- training$classe
typeColor[typeColor =="A"] <- 1
typeColor[typeColor =="B"] <- 2
typeColor[typeColor =="C"] <- 3
typeColor[typeColor =="D"] <- 4
typeColor[typeColor =="E"] <- 5

## PCA- full set of variables for
prcomp <- prcomp(training[,-53])
#prcomp$rotation
#str(prcomp)
#prcomp

plot(prcomp$x[,1], prcomp$x[,2], col=typeColor, xlab="PC1",ylab="PC2", cex=1)

## Remove predictors that have little to know relationship with outcome variable after eda
 training2 <- subset(training, select=-c(gyros_belt_x,gyros_belt_y,gyros_belt_z,gyros_arm_x,gyros_arm_y,gyros_arm_z,
                                        gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,gyros_forearm_x,gyros_forearm_y,gyros_forearm_z) )

 testing2 <- subset(testing.clean, select=-c(gyros_belt_x,gyros_belt_y,gyros_belt_z,gyros_arm_x,gyros_arm_y,gyros_arm_z,
                              gyros_dumbbell_x,gyros_dumbbell_y,gyros_dumbbell_z,gyros_forearm_x,gyros_forearm_y,gyros_forearm_z) )
 str(training2)
 str(testing2)
```


**Modeling**
 
For the exploratory data analysis the data was partitioned into training and test sets (75%/25% split). I continued on with this split creating three subsamples for 3-fold cross validation.  One run of the cross validation is highlighted in this section but all three were run and the results summarized below.  The training and test sets had 14,718 and 4,904 observations with 60 variables, respectively.  After removing the predictors starting with “gyro” and seven additional character, timestamp and/or id variables I was left with 41 predictors to train/test on.

I built three types of models, RPART- classification and regression tree (see figure 4), RF- random forest and GBM- boosting with trees.  I also stacked these models to see how an ensemble model performed.

Figure 4- classification and regression tree
```{r}
#### Rpart- classification and regression tree
mod.rpart <- train(classe ~ ., data=training, method="rpart")
mod.rpart$finalModel

## Plot Classification and Regression Tree
  par(mar=c(1,4,1,4))
  plot(mod.rpart$finalModel, uniform=TRUE, main="Classification Tree", cex.main=0.75)
  text(mod.rpart$finalModel, use.n=TRUE, all=TRUE, cex=0.7,pos=1)
  
## Get predictions and confusion(accuracy matrix) for rpart
pred.rpart <- predict(mod.rpart, testing)
confusionMatrix(testing$classe, pred.rpart)
```


```{r}
#### RF- Random Forest
mod.rf <- train(classe ~ ., data=training2, method="rf")
mod.rf$finalModel

## Get predictions and confusion(accuracy matrix) for rf
pred.rf <- predict(mod.rf, testing2)
confusionMatrix(testing2$classe, pred.rf)
```



```{r}
#### GBM- boosting with trees
mod.gbm <- train(classe ~ ., data=training2, method="gbm", verbose=FALSE)
mod.gbm$finalModel

## get predictions and confusion(accuracy matrix) for gbm
pred.gbm <- predict(mod.gbm, testing2)
confusionMatrix(testing2$classe, pred.gbm)
```


```{r}
#### Ensemble Model
## Stack RPART, RF and GBM models
stacked.dat <- data.frame(pred.rpart, pred.rf, pred.gbm, classe=testing2$classe)
str(stacked.dat)

## Train the combined stacked predictors using random forests
mod.stack  <- train(classe ~., method="rf", data=stacked.dat)

## Get predictions and confusion(accuracy matrix) for stacked models
pred.stack <- predict(mod.stack, stacked.dat)
confusionMatrix(testing2$classe, pred.stack)
```


**Results and predictions**

Below is the result table with my out of sample accuracy.  The Random Forest and Boosting models performed much better than the Classification and Regression Tree model.  The ensemble model was also very accurate.  I used the RF model to make my final predictions.  The predictions were the same using the RF model from any of the three samples.  These predictions scored a 100% on the final prediction quiz.  

Table of Out of Sample Accuracy
```{r}
readRDS(file="/Users/cmay/Documents/Training/finalresults.rds")
```



Final predictions on the provided 20 test cases,
```{r}
##### Final Predictions
## Make final predictions for end of project quiz using Random Forests as it had the best out of sample accuracy
final.prediction <- predict(mod.rf, pmltest.clean)
final.prediction
summary(final.prediction)
```

**Predictions**

Sample 1-RF	B A B A A E D B A A B C B A E E A B B B		
Sample 2-RF	B A B A A E D B A A B C B A E E A B B B		
Sample 3-RF	B A B A A E D B A A B C B A E E A B B B		




